{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp3XPuaTu9jl"
   },
   "source": [
    "\n",
    "# How to generate text: using different decoding methods for language generation with Transformers\n",
    "\n",
    "(based on https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxLvv6UaPa33"
   },
   "source": [
    "### **Introduction**\n",
    "\n",
    "In recent years, there has been an increasing interest in open-ended language generation thanks to the rise of large transformer-based language models trained on millions of webpages, such as OpenAI's famous [GPT2 model](https://openai.com/blog/better-language-models/). The results on conditioned open-ended language generation are impressive, e.g. [GPT2 on unicorns](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [Controlled language with CTRL](https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/). Besides the improved transformer architecture and massive unsupervised training data, **better decoding methods** have also played an important role.\n",
    "\n",
    "This blog post gives a brief overview of different decoding strategies and more importantly shows how *you* can implement them with very little effort using the popular `transformers` library!\n",
    "\n",
    "All of the following functionalities can be used for **auto-regressive** language generation ([here](http://jalammar.github.io/illustrated-gpt2/) a refresher). In short, *auto-regressive* language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions:\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "and $W_0$ being the initial *context* word sequence. The length $T$ of the word sequence is usually determined *on-the-fly* and corresponds to the timestep $t=T$ the EOS token is generated from $P(w_{t} | w_{1: t-1}, W_{0})$.\n",
    "\n",
    "\n",
    "Auto-regressive language generation is now available for `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` in both PyTorch and Tensorflow >= 2.0!\n",
    "\n",
    "We will give a tour of the currently most prominent decoding methods, mainly *Greedy search*, *Beam search*, *Top-K sampling* and *Top-p sampling*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si4GyYhOQMzi"
   },
   "source": [
    "Let's quickly install transformers and load the model. ~~We will use GPT2 in Tensorflow 2.1 for demonstration, but the API is 1-to-1 the same for PyTorch.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbzZ_IVTtoQe",
    "outputId": "2ad89708-539c-44b1-8b68-345e81d11947"
   },
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/huggingface/transformers.git  # the bleeding edge version\n",
    "#!pip install -q tensorflow==2.1\n",
    "# !pip install transformers==4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eL5Ky2ESXOzz"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "a83b8f9b1f2c4a969a7fd88b0fb03f3d",
      "b543d8c68d574b7b9f764a538c444852",
      "9038969487d642b3a2704aba4843d3b3",
      "2b99d1ba540c4983adfcea51adcb5db2",
      "afb93824648b452d8f2147ad809d97ac",
      "a1d4f161119046dbb40edcedf5927676",
      "866892ff7f554eb1ab81c0a258eb9ccd",
      "8de8fcde63d8447e82781055e8a241b9",
      "5930805e0beb4219827c2f3511f23f73",
      "f628e363572b4becaec55d3e50613f6a",
      "38e9b00bca404e3a9dda116b17b4c999",
      "b2e44393e3f44093a58a546f70699163",
      "42ea6a486f4b416596d067030742c7e7",
      "99a5dac513a446df8a20174b05c2f11c",
      "6ba11543e31244f3a37d15cd5940014e",
      "9d65ebdcdf8044748c6c61313dd91679",
      "5359132617304f3abee2fd2d6e27af63",
      "2997bdf4ce4a4d90864590978287b8ed",
      "6f244063036d484c9f4c71763eefdb6c",
      "9f30946308c8412aac33677257cee360",
      "9a5ef54f74da49e6acb2a48aa2442fa0",
      "dfcf5044ac9047c3b63d9947403a9a8f",
      "d4202bd4d19c4a2fb8355f955dc384be",
      "6b62afff3a9d464bb07effbcd7e49ad3",
      "0a4a8a95730a4c39ae061567ab9aa27d",
      "4dd183fdd1124345b301a1ea621c5d33",
      "e3b326b7800c494b8b79122589ae8d77",
      "b84d5ec879ea4a868f472f8e807b2e96",
      "530348026f9d4cd6898bbf5ee4ca367b",
      "10218ebb34af4d36bf4b7d7342fb5ca4",
      "1be26038ed46442eae0a99b9439c1958",
      "0ac877cd73b74fc7b65a44820deaa733",
      "409c75a68c824fc080ff743f11cbc314",
      "f3a9904fd83d43958d6bc1eca7d390d7",
      "5700b22a9f8944e4b0ba7aff1091fe68",
      "adadd11e13f94f0486a7d9e4c4ae7c15",
      "f5d28ac1e3fa414d9e7641f00ac514b3",
      "599e273e0c794e0ab10f369c5c49ce6d",
      "cfd7bfbac25f4fcbbfdb065b0181f553",
      "313c3de1b62b4d23ab1786133b081a80",
      "0af8b918d8304be895223ab28852e27b",
      "32b908b4a36f491a849a64d56a5f209d",
      "4af0ce7bac7f4a52b9b2088e63a99ce8",
      "00c2d42295c34646afa689f1695cbae8",
      "4d2f374fca434888b70f06673c624206",
      "9dfa0e7c559447ef9eea100745f44c89",
      "c64088e1b5294763a6a520fcc703d758",
      "11f2b9cbd7e841b288b4bbc894497644",
      "b0d68c965c50411a9207d0a359867c79",
      "ce08e20a96c248e28c47bafab7f3c0de",
      "0b7f1a6c3b6049329fadaf9f02cec518",
      "7c56450af9ea4ffcbff1f851149ec043",
      "f554925e9d5448ada54ad2a4dffe9440",
      "27d5f185fa3d475b8e79eefe13060ce7",
      "9f339e58ac72497b8bfbfe75a186b74a"
     ]
    },
    "id": "ue2kOQhXTAMU",
    "outputId": "d2cc6acb-84b1-46d8-fbf5-4326ace26368"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8Y7cgu9ohXP"
   },
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. The following sketch shows greedy search.\n",
    "\n",
    "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
    "\n",
    "Starting from the word $\\text{\"The\"}$, the algorithm\n",
    "greedily chooses the next word of highest probability $\\text{\"nice\"}$ and so on, so that the final generated word sequence is $\\text{\"The\", \"nice\", \"woman\"}$ having an overall probability of $0.5 \\times 0.4 = 0.2$.\n",
    "\n",
    "In the following we will generate word sequences using GPT2 on the context $(\\text{\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"})$. Let's see how greedy search can be used in `transformers` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWLd_J6lXz_t",
    "outputId": "930e9d0d-2879-40da-ec4c-c8aba8311be4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I can do it with my cat. I'm not sure if I can do it with my cat.\n",
      "\n",
      "I'm not sure if I can do it with my cat.\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBn1ePmJvhrl"
   },
   "source": [
    "Alright! We have generated our first short text with GPT2 ðŸ˜Š. The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search - check out [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) and [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n",
    "\n",
    "The major drawback of greedy search though is that it misses high probability words hidden behind a low probability word as can be seen in our sketch above:\n",
    "\n",
    "The word $\\text{\"has\"}$ with its high conditional probability of $0.9$ is hidden behind the word $\\text{\"dog\"}$, which has only the second-highest conditional probability, so that greedy search misses the word sequence $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$.\n",
    "\n",
    "Thankfully, we have beam search to alleviate this problem!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8DnXZ1WiuNd"
   },
   "source": [
    "### **Beam search**\n",
    "\n",
    "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Let's illustrate with `num_beams=2`:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "At time step $1$, besides the most likely hypothesis $\\text{\"The\", \"woman\"}$, beam search also keeps track of the second most likely one $\\text{\"The\", \"dog\"}$. At time step $2$, beam search finds that the word sequence $\\text{\"The\", \"dog\", \"has\"}$ has with $0.36$ a higher probability than $\\text{\"The\", \"nice\", \"woman\"}$, which has $0.2$. Great, it has found the most likely word sequence in our toy example!\n",
    "\n",
    "Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n",
    "\n",
    "Let's see how beam search can be used in `transformers`. We set `num_beams > 1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1R5kx30Ynej",
    "outputId": "08bda913-d983-4224-a036-a53d08ae2d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I don't want to do it every day. I want to do it once in a while, but I don't want to do it every day. I want to do it once in a while,\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ6xs-KLi9jT"
   },
   "source": [
    "While the result is arguably more fluent, the output still includes repetitions of the same word sequences.  \n",
    "A simple remedy is to introduce *n-grams* (*a.k.a* word sequences of $n$ words) penalties as introduced by [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) and [Klein et al. (2017)](https://arxiv.org/abs/1701.02810). The most common *n-grams* penalty makes sure that no *n-gram* appears twice by manually setting the probability of next words that could create an already seen *n-gram* to $0$.\n",
    "\n",
    "Let's try it out by setting `no_repeat_ngram_size=2` so that no *2-gram* appears twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy3iVJgfnkMi",
    "outputId": "c878b88f-ca9f-43f2-d0c4-c983c5e001da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog walker.\n",
      "\n",
      "I started with a simple design. I wanted to create something that I could\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxsksOGDpmA0"
   },
   "source": [
    "Nice, that looks much better! We can see that the repetition does not appear anymore. Nevertheless, *n-gram* penalties have to be used with care. An article generated about the city *New York* should not use a *2-gram* penalty or otherwise, the name of the city would only appear once in the whole text!\n",
    "\n",
    "Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best.\n",
    "\n",
    "In `transformers`, we simply set the parameter `num_return_sequences` to the number of highest scoring beams that should be returned. Make sure though that `num_return_sequences <= num_beams`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ClO3VphqGp6",
    "outputId": "d5bb2959-9e0e-45f5-93df-a954204a0272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog walker.\n",
      "\n",
      "I started with a simple design. I wanted to create something that I could\n",
      "\n",
      "1: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog walker.\n",
      "\n",
      "I started with a simple design. I wanted to create something that could be\n",
      "\n",
      "2: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog walker.\n",
      "\n",
      "I started with a simple design. I wanted to create something that would be\n",
      "\n",
      "3: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog walker.\n",
      "\n",
      "I started with a simple design. I wanted to create a device that would\n",
      "\n",
      "4: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog walker.\n",
      "\n",
      "I started with a simple design. I wanted to create something that was easy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTAtnY_cek9m"
   },
   "source": [
    "As can be seen, the five beam hypotheses are only marginally different to each other - which should not be too surprising when using only 5 beams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FTLZ5ajedK3"
   },
   "source": [
    "## Diverse beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPa_ZSBheluH",
    "outputId": "6476631b-cd72-47d9-9156-8c9704682bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog and she loves it. She is very friendly and loves to play with me. I am very happy with the purchase and I will definitely be buying more of these.\n",
      "\n",
      "Rated 5 out of 5 by Anonymous from\n",
      "\n",
      "1: I enjoy walking with my cute dog, and I love to play with her. I'm not sure why I've been so reluctant to share this with you, but I think it's because I don't want to be judged for it.\n",
      "\n",
      "\n",
      "\n",
      "2: I enjoy walking with my cute dog. I love to play with her. She is so cute and I am so happy to have her with me.\n",
      "\n",
      "I love the fact that I can go to the park with the dog and she can play\n",
      "\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I can do it with a dog that's so big. I've been thinking about it, and I think I'll try it.\n",
      "\n",
      "I'm going to try to get\n",
      "\n",
      "4: I enjoy walking with my cute dog. She is so cute and I love to see her happy. I have been to the vet twice and she has been fine. My dog is a little bit sick and has had a few infections. We have had\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    num_beam_groups=5, # this must be a divisor of num_beams\n",
    "    diversity_penalty=1.0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhLKyfdbsjXc"
   },
   "source": [
    "In open-ended generation, a couple of reasons have recently been brought forward why beam search might not be the best possible option:\n",
    "\n",
    "- Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization - see [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) and [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n",
    "\n",
    "- We have seen that beam search heavily suffers from repetitive generation. This is especially hard to control with *n-gram*- or other penalties in story generation since finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical *n-grams* requires a lot of finetuning.\n",
    "\n",
    "- As argued in [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751), high quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. The authors show this nicely by plotting the probability, a model would give to human text vs. what beam search does.\n",
    "\n",
    "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
    "\n",
    "\n",
    "So let's stop being boring and introduce some randomness ðŸ¤ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbbIyK84wHq6"
   },
   "source": [
    "## **Sampling**\n",
    "\n",
    "In its most basic form, sampling means randomly picking the next word $w_t$ according to its conditional probability distribution:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "Taking the example from above, the following graphic visualizes language generation when sampling.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "It becomes obvious that language generation using sampling is not *deterministic* anymore. The word\n",
    "$\\text{\"car\"}$ is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling $\\text{\"drives\"}$ from $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n",
    "\n",
    "In `transformers`, we set `do_sample=True` and deactivate *Top-K* sampling (more on this later) via `top_k=0`. In the following, we will fix `random_seed=0` for illustration purposes. Feel free to change the `random_seed` to play around with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRAz4D-Ks0_4",
    "outputId": "ec4a87f6-fbbb-4f63-d96c-64a41aa4d4cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog when the sun is out (Exact same thing when I am not wearing a bikini or sticky palm), so when we visited the beach that 95% of the activities were women-only. The beaches were safe enough\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQHuo911wfT-"
   },
   "source": [
    "Interesting! The text seems alright - but when taking a closer look, it is not very coherent. the *3-grams* *new hand sense* and *local batte harness* are very weird and don't sound like they were written by a human. That is the big problem when sampling word sequences: The models often generate incoherent gibberish, *cf.* [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751).\n",
    "\n",
    "A trick is to make the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n",
    "\n",
    "$q = \\frac{exp(z_i / T)}{\\sum_j exp(z_j / T)}$\n",
    "\n",
    "An illustration of applying temperature to our example from above could look as follows.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
    "\n",
    "The conditional next word distribution of step $t=1$ becomes much sharper leaving almost no chance for word $\\text{\"car\"}$ to be selected.\n",
    "\n",
    "\n",
    "Let's see how we can cool down the distribution in the library by setting `temperature=0.7`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgJredc-0j0Z",
    "outputId": "65e0d9f9-76a5-4e31-cbb2-f4f3c3bd398c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, so you can imagine how I must feel when I see these adorable puppies in my arms!\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzGuu24hZZnq"
   },
   "source": [
    "OK. There are less weird n-grams and the output is a bit more coherent now! While applying temperature can make a distribution less random, in its limit, when setting `temperature` $ \\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binNTroyzQBu"
   },
   "source": [
    "## **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) introduced a simple, but very powerful sampling scheme, called ***Top-K*** sampling. In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words.\n",
    "GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation.\n",
    "\n",
    "We extend the range of words used for both sampling steps in the example above from 3 words to 10 words to better illustrate *Top-K* sampling.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only *ca.* two-thirds of the whole probability mass in the first step, it includes almost all of the probability mass in the second step. Nevertheless, we see that it successfully eliminates the rather weird candidates $\\text{\"not\", \"the\", \"small\", \"told\"}$\n",
    "in the second sampling step.\n",
    "\n",
    "\n",
    "Let's see how *Top-K* can be used in the library by setting `top_k=50`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBtDOdD0wx3l",
    "outputId": "31bdf71f-5749-41ac-ab18-c1d8d47447c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog or riding my bike with my friends as long as I can avoid a traffic jam. However, when I'm driving, I'm in control. If I find myself in a ditch without a backup, or in a\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77H5m4ZmhEX"
   },
   "source": [
    "Not bad at all! The text is arguably the most *human-sounding* text so far.\n",
    "One concern though with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$.\n",
    "This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above).\n",
    "\n",
    "In step $t=1$, *Top-K* eliminates the possibility to\n",
    "sample $\\text{\"people\", \"big\", \"house\", \"cat\"}$, which seem like reasonable candidates. On the other hand, in step $t=2$ the method includes the arguably ill-fitted words $\\text{\"down\", \"a\"}$ in the sample pool of words. Thus, limiting the sample pool to a fixed size *K* could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution.\n",
    "This intuition led [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) to create ***Top-p***- or ***nucleus***-sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki9LAaexzV3H"
   },
   "source": [
    "## **Top-p (nucleus) sampling**\n",
    "\n",
    "Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words. This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92\\%$ of the probability mass, defined as $V_{\\text{top-p}}$. In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. Quite simple actually! It can be seen that it keeps a wide range of words where the next word is arguably less predictable, *e.g.* $P(w | \\text{\"The\"})$, and only a few words when the next word seems more predictable, *e.g.* $P(w | \\text{\"The\", \"car\"})$.\n",
    "\n",
    "Alright, time to check it out in `transformers`!\n",
    "We activate *Top-p* sampling by setting `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvwIc7YAx77F",
    "outputId": "8034ba73-7d83-4ab5-e9d3-6f47a8c879ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog Ethereal.\" The owner\n",
      "\n",
      "Jillian said, \"Our pet started showing us signs of autism and eventually we dropped him off. In all honesty, I think he loves her more and will give her love again\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(3)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn-8gLaR4lat"
   },
   "source": [
    "Great, that sounds like it could have been written by a human. Well, maybe not quite yet.\n",
    "\n",
    "While in theory, *Top-p* seems more elegant than *Top-K*, both methods work  well in practice. *Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some dynamic selection.\n",
    "\n",
    "Finally, to get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kY8P9VG8Gi9",
    "outputId": "3d8d3ba5-eb65-4a81-beb6-c9542af06942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog or riding my bike with my friends as long as I can avoid a traffic jam. However, when I'm driving, I'm in control. If I have to pass, pass before I pass. If I have\n",
      "1: I enjoy walking with my cute dog. I love my dog. She's my little friend who I love dearly.\n",
      "\n",
      "I'll go with her when she has her lunch, but I've also done some errands and I know she loves a\n",
      "2: I enjoy walking with my cute dog, but I would definitely encourage anyone who has a puppy to adopt a miniature and have her walk with him.\n",
      "\n",
      "Why did you start making pictures of pets?\n",
      "\n",
      "Well I wanted to make something from scratch\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vRPfMl88rk0"
   },
   "source": [
    "Cool, now you should have all the tools to let your model write your stories with `transformers`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZpN3QTweA0J"
   },
   "source": [
    "## Constrained generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYsyCeBUJAVy"
   },
   "source": [
    "For better readability, we will use end-of-line as the EOS token, instead of just always generating 50 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUK1B7WsJCpM",
    "outputId": "1b75f284-a236-443e-d6cd-84702cfaa809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "END_OF_LINE = tokenizer('\\n').input_ids[0]\n",
    "print(END_OF_LINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBATNPrxJwPB"
   },
   "source": [
    "The baseline beam search produces very similar sentences; all of them contain \"not sure\" or \"don't think\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgsoFsmOI9Pe",
    "outputId": "ddd11a49-4c62-45f1-eb64-bc8c668ef344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I don't have the time to do it every day.\n",
      "\n",
      "1: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own dog food.\n",
      "\n",
      "2: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make this recipe for you.\n",
      "\n",
      "3: I enjoy walking with my cute dog, but I don't have the time to do it every day. So, I decided to make my own.\n",
      "\n",
      "4: I enjoy walking with my cute dog, but I don't want to do it every day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj3S7FkgI9lv"
   },
   "source": [
    "What would happen if we forbid the model to use these phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-nlHOYzJ_bP",
    "outputId": "d5e6d14d-a84e-4144-f3dc-0064b4a2e66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her without having to worry about her getting hurt.\n",
      "\n",
      "1: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her without having to worry about her getting lost.\n",
      "\n",
      "2: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her without having to worry about her getting sick.\n",
      "\n",
      "3: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her without having to worry about her running away.\n",
      "\n",
      "4: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to take her out for a walk when she wants to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    "    bad_words_ids=tokenizer(['sure', 'think', 'thundersnatch'], add_prefix_space=True)['input_ids'],\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqjdJiyVMIAm"
   },
   "source": [
    "Why `add_prefix space`? Because the BPE tokenization used by GPT prepends the space to the next word, and this changes the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ng2xaRxaMSXS",
    "outputId": "2cda3025-8db4-47ad-ad37-b57647f308cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19532], [1654], [314, 716, 407, 1654]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['sure', ' sure', ' I am not sure'])['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J16fr3qDKOz3"
   },
   "source": [
    "We can see that the meaning of these texts has changed a lot â€“ but in some unpredictable way.\n",
    "\n",
    "Can we force the model to write a text involving cats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsOq5q5PKspB",
    "outputId": "dc4600a8-7128-47ee-d8aa-efe6313a6388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her and have her play with the other dogs. So I decided to make\n",
      "1: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her and have her play with the other dogs. So I decided to create\n",
      "2: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her and have her play with the other dogs. So I decided to build\n",
      "3: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her and have her play with the other dogs. So I decided to cat\n",
      "4: I enjoy walking with my cute dog, but I don't want her to have to walk with me all the time. I want to be able to go to the park with her and have her play with the other dogs. So I've been cat\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    "    bad_words_ids=tokenizer(['sure', 'think'], add_prefix_space=True)['input_ids'],\n",
    "    force_words_ids=[tokenizer(['cat'], add_prefix_space=True, add_special_tokens=False).input_ids],\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40lA3WrhK726"
   },
   "source": [
    "A clarification: **force_words_ids** is a list of constraints. Each constraint is a list of expressions, such that at least one expression should be included into the generated text. And each expression is just a list of tokens.\n",
    "\n",
    "See the discussion in [the HF pull request](https://github.com/huggingface/transformers/issues/14081), or read the paper \"[Guided Generation of Cause and Effect](https://www.ijcai.org/proceedings/2020/0502.pdf)\" by Li et al, where the algorithm was proposed.  \n",
    "\n",
    "To evaluate the power of these constraints, let us force the model to include a mouse (or even many mice) into the text. We can also relax the \"cat\" constraint by allowing the words \"cats\", \"kitten\" or \"feline\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXvaYLFFLQWT",
    "outputId": "3a8eb76d-751d-4645-9969-9ef25c8cbfbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog. I love to play with her. She is so cute and I am so happy to have her with me. My husband and my daughter are very happy with the dog and the house. We are so grateful for\n",
      "1: I enjoy walking with my cute dog. I love to play with her. She is so cute and I am so happy to have her with me. My husband and my daughter are very happy with the dog and the cat. They love the dogs and\n",
      "2: I enjoy walking with my cute dog. I love to play with her. She is so cute and I am so happy to have her with me. My husband and my daughter are very happy with the dog and the house. We are so grateful f\n",
      "3: I enjoy walking with my cute dog. I love to play with her. She is so cute and I am so happy to have her with me. My husband and my daughter are very happy with the dog and the cat. They love the dogs mouse\n",
      "4: I enjoy walking with my cute dog. I love to play with her. She is so cute and I am so happy to have her with me. My husband and my daughter are very happy with the dog and the cat. They love the dogs mice\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    "    bad_words_ids=tokenizer(['sure', 'think'], add_prefix_space=True)['input_ids'],\n",
    "    force_words_ids = [\n",
    "        tokenizer(['cat', 'cats', 'kitten', 'feline', 'Cat', 'Cats'], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "        tokenizer(['mouse', 'mice'], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkkfGBd9L6uL"
   },
   "source": [
    "The texts satisfy the constraints and look fluent. Still, the model has somehow fooled us: it used the verb \"to mouse\" in its secondary sense, instead of referring to animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsWd7e98Vcs3"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "As *ad-hoc* decoding methods, *top-p* and *top-K* sampling seem to produce more fluent text than traditional *greedy* - and *beam* search on open-ended language generation.\n",
    "Recently, there has been more evidence though that the apparent flaws of *greedy* and *beam* search - mainly generating repetitive word sequences - are  caused by the model (especially the way the model is trained), rather than the decoding method, *cf.* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Also, as demonstrated in [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492), it looks as *top-K* and *top-p* sampling also suffer from generating repetitive word sequences.\n",
    "\n",
    "In [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf), the authors show that according to human evaluations, *beam* search can generate more fluent text than *Top-p* sampling, when adapting the model's training objective.\n",
    "\n",
    "Open-ended language generation is a rapidly evolving field of research and as it is often the case there is no one-size-fits-all method here, so one has to see what works best in one's specific use case.\n",
    "\n",
    "Good thing, that *you* can try out all the different decoding methods in `transfomers` ðŸ¤—.\n",
    "\n",
    "That was a short introduction on how to use different decoding methods in `transformers` and recent trends in open-ended language generation.\n",
    "\n",
    "Feedback and questions are very welcome on the [Github repository](https://github.com/huggingface/transformers).\n",
    "\n",
    "For more fun generating stories, please take a look at [Writing with Transformers](https://transformer.huggingface.co).\n",
    "\n",
    "Thanks to everybody, who has contributed to the blog post: Alexander Rush, Julien Chaumand, Thomas Wolf, Victor Sanh, Sam Shleifer, ClÃ©ment Delangue, Yacine Jernite, Oliver Ã…strand and John de Wasseige.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4CYi91h11yd"
   },
   "source": [
    "## **Appendix**\n",
    "\n",
    "There are a couple of additional parameters for the `generate` method that were not mentioned above. We will explain them here briefly!\n",
    "\n",
    "- `min_length` can be used to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n",
    "- `repetition_penalty` can be used to penalize words that were already generated or belong to the context. It was first introduced by [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) and is also used in the training objective in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, *e.g.* see this [discussion](https://github.com/huggingface/transformers/pull/2303) on Github.\n",
    "\n",
    "- `attention_mask` can be used to mask padded tokens\n",
    "- `pad_token_id`, `bos_token_id`, `eos_token_id`: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n",
    "\n",
    "For more information please also look into the `generate` function [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using function model.generate() from seminar, provide examples of generated texts with regular samplings(without top-p and top-k) with 0.0007 and 4.6 temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Temperature 0.0007:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I can do it with a dog that's so big. I've been thinking about it, and I think I'll try it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ð¡ Ð½Ð¸Ð·ÐºÐ¾Ð¹ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð¾Ð¹ (0.0007)\n",
    "Ð˜Ð· Ñ‚Ð¾Ð³Ð¾ Ñ‡Ñ‚Ð¾ Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ÑÑ:\n",
    "Ð’Ñ‹Ð²Ð¾Ð´ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾Ñ…Ð¾Ð¶ Ð½Ð° Ð·Ð°ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð²Ð°Ñ€Ð¸Ð°Ñ†Ð¸ÑÐ¼Ð¸.\n",
    "ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð±ÑƒÐ´ÐµÑ‚ Ð±Ð¾Ð»ÐµÐµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·ÑƒÐµÐ¼Ð¾Ð¹ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¹, Ð½Ð¾ Ñ‚ÐµÐºÑÑ‚ Ð¼Ð¾Ð¶ÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑŒ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Â«ÑÑƒÑ…Ð¸Ð¼Â» Ð¸ Ð·Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼.\n",
    "\"\"\"\n",
    "output_low_temp = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    top_k=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.0007,\n",
    "    no_repeat_ngram_size=2,\n",
    "    eos_token_id=END_OF_LINE,\n",
    ")\n",
    "print(\"Output with Temperature 0.0007:\\n\" + \"-\" * 100)\n",
    "print(tokenizer.decode(output_low_temp[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Temperature 4.6:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog...we've become really, really attached and together...\"So here you have everything the world of video will hate about YouPorn, while our world still seems to find some room! Just one question...\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ð¡ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð¾Ð¹ (4.6)\n",
    "Ð˜Ð· Ñ‚Ð¾Ð³Ð¾ Ñ‡Ñ‚Ð¾ Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ÑÑ:\n",
    "Ð’Ñ‹Ð²Ð¾Ð´ Ð±ÑƒÐ´ÐµÑ‚ Ð¼ÐµÐ½ÐµÐµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·ÑƒÐµÐ¼Ñ‹Ð¼, Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ñ‚ÐµÐºÑÑ‚Ð° Ð¼Ð¾Ð¶ÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑŒ Ð½ÐµÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾Ð¹.\n",
    "ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ð°Ñ‡Ð½ÐµÑ‚ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð², Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¿Ð¾ÑÐ²Ð¸Ñ‚ÑŒÑÑ Ð½Ðµ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ð´Ñ€ÑƒÐ³ Ñ Ð´Ñ€ÑƒÐ³Ð¾Ð¼ Ñ„Ñ€Ð°Ð·Ñ‹.\n",
    "\"\"\" \n",
    "\n",
    "output_high_temp = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    top_k=30,\n",
    "    do_sample=True,\n",
    "    temperature=4.6,\n",
    "    no_repeat_ngram_size=2,\n",
    "    eos_token_id=END_OF_LINE,\n",
    ")\n",
    "print(\"Output with Temperature 4.6:\\n\" + \"-\" * 100)\n",
    "print(tokenizer.decode(output_high_temp[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c2d42295c34646afa689f1695cbae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a4a8a95730a4c39ae061567ab9aa27d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10218ebb34af4d36bf4b7d7342fb5ca4",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1be26038ed46442eae0a99b9439c1958",
      "value": 26
     }
    },
    "0ac877cd73b74fc7b65a44820deaa733": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0af8b918d8304be895223ab28852e27b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b7f1a6c3b6049329fadaf9f02cec518": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10218ebb34af4d36bf4b7d7342fb5ca4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11f2b9cbd7e841b288b4bbc894497644": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27d5f185fa3d475b8e79eefe13060ce7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9f339e58ac72497b8bfbfe75a186b74a",
      "value": "â€‡1.42G/1.42Gâ€‡[00:30&lt;00:00,â€‡77.8MB/s]"
     }
    },
    "1be26038ed46442eae0a99b9439c1958": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27d5f185fa3d475b8e79eefe13060ce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2997bdf4ce4a4d90864590978287b8ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b99d1ba540c4983adfcea51adcb5db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f628e363572b4becaec55d3e50613f6a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_38e9b00bca404e3a9dda116b17b4c999",
      "value": "â€‡0.99M/0.99Mâ€‡[00:00&lt;00:00,â€‡1.06MB/s]"
     }
    },
    "313c3de1b62b4d23ab1786133b081a80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32b908b4a36f491a849a64d56a5f209d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "38e9b00bca404e3a9dda116b17b4c999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "409c75a68c824fc080ff743f11cbc314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42ea6a486f4b416596d067030742c7e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5359132617304f3abee2fd2d6e27af63",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2997bdf4ce4a4d90864590978287b8ed",
      "value": "Downloading:â€‡100%"
     }
    },
    "4af0ce7bac7f4a52b9b2088e63a99ce8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d2f374fca434888b70f06673c624206": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9dfa0e7c559447ef9eea100745f44c89",
       "IPY_MODEL_c64088e1b5294763a6a520fcc703d758",
       "IPY_MODEL_11f2b9cbd7e841b288b4bbc894497644"
      ],
      "layout": "IPY_MODEL_b0d68c965c50411a9207d0a359867c79"
     }
    },
    "4dd183fdd1124345b301a1ea621c5d33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ac877cd73b74fc7b65a44820deaa733",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_409c75a68c824fc080ff743f11cbc314",
      "value": "â€‡26.0/26.0â€‡[00:00&lt;00:00,â€‡1.68kB/s]"
     }
    },
    "530348026f9d4cd6898bbf5ee4ca367b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5359132617304f3abee2fd2d6e27af63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5700b22a9f8944e4b0ba7aff1091fe68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfd7bfbac25f4fcbbfdb065b0181f553",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_313c3de1b62b4d23ab1786133b081a80",
      "value": "Downloading:â€‡100%"
     }
    },
    "5930805e0beb4219827c2f3511f23f73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "599e273e0c794e0ab10f369c5c49ce6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b62afff3a9d464bb07effbcd7e49ad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b84d5ec879ea4a868f472f8e807b2e96",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_530348026f9d4cd6898bbf5ee4ca367b",
      "value": "Downloading:â€‡100%"
     }
    },
    "6ba11543e31244f3a37d15cd5940014e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a5ef54f74da49e6acb2a48aa2442fa0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dfcf5044ac9047c3b63d9947403a9a8f",
      "value": "â€‡446k/446kâ€‡[00:00&lt;00:00,â€‡785kB/s]"
     }
    },
    "6f244063036d484c9f4c71763eefdb6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c56450af9ea4ffcbff1f851149ec043": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "866892ff7f554eb1ab81c0a258eb9ccd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8de8fcde63d8447e82781055e8a241b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9038969487d642b3a2704aba4843d3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8de8fcde63d8447e82781055e8a241b9",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5930805e0beb4219827c2f3511f23f73",
      "value": 1042301
     }
    },
    "99a5dac513a446df8a20174b05c2f11c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f244063036d484c9f4c71763eefdb6c",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f30946308c8412aac33677257cee360",
      "value": 456318
     }
    },
    "9a5ef54f74da49e6acb2a48aa2442fa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d65ebdcdf8044748c6c61313dd91679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dfa0e7c559447ef9eea100745f44c89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce08e20a96c248e28c47bafab7f3c0de",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b7f1a6c3b6049329fadaf9f02cec518",
      "value": "Downloading:â€‡100%"
     }
    },
    "9f30946308c8412aac33677257cee360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f339e58ac72497b8bfbfe75a186b74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1d4f161119046dbb40edcedf5927676": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a83b8f9b1f2c4a969a7fd88b0fb03f3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b543d8c68d574b7b9f764a538c444852",
       "IPY_MODEL_9038969487d642b3a2704aba4843d3b3",
       "IPY_MODEL_2b99d1ba540c4983adfcea51adcb5db2"
      ],
      "layout": "IPY_MODEL_afb93824648b452d8f2147ad809d97ac"
     }
    },
    "adadd11e13f94f0486a7d9e4c4ae7c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0af8b918d8304be895223ab28852e27b",
      "max": 718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32b908b4a36f491a849a64d56a5f209d",
      "value": 718
     }
    },
    "afb93824648b452d8f2147ad809d97ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0d68c965c50411a9207d0a359867c79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2e44393e3f44093a58a546f70699163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42ea6a486f4b416596d067030742c7e7",
       "IPY_MODEL_99a5dac513a446df8a20174b05c2f11c",
       "IPY_MODEL_6ba11543e31244f3a37d15cd5940014e"
      ],
      "layout": "IPY_MODEL_9d65ebdcdf8044748c6c61313dd91679"
     }
    },
    "b543d8c68d574b7b9f764a538c444852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1d4f161119046dbb40edcedf5927676",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_866892ff7f554eb1ab81c0a258eb9ccd",
      "value": "Downloading:â€‡100%"
     }
    },
    "b84d5ec879ea4a868f472f8e807b2e96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c64088e1b5294763a6a520fcc703d758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c56450af9ea4ffcbff1f851149ec043",
      "max": 1520013706,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f554925e9d5448ada54ad2a4dffe9440",
      "value": 1520013706
     }
    },
    "ce08e20a96c248e28c47bafab7f3c0de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfd7bfbac25f4fcbbfdb065b0181f553": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4202bd4d19c4a2fb8355f955dc384be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b62afff3a9d464bb07effbcd7e49ad3",
       "IPY_MODEL_0a4a8a95730a4c39ae061567ab9aa27d",
       "IPY_MODEL_4dd183fdd1124345b301a1ea621c5d33"
      ],
      "layout": "IPY_MODEL_e3b326b7800c494b8b79122589ae8d77"
     }
    },
    "dfcf5044ac9047c3b63d9947403a9a8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3b326b7800c494b8b79122589ae8d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3a9904fd83d43958d6bc1eca7d390d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5700b22a9f8944e4b0ba7aff1091fe68",
       "IPY_MODEL_adadd11e13f94f0486a7d9e4c4ae7c15",
       "IPY_MODEL_f5d28ac1e3fa414d9e7641f00ac514b3"
      ],
      "layout": "IPY_MODEL_599e273e0c794e0ab10f369c5c49ce6d"
     }
    },
    "f554925e9d5448ada54ad2a4dffe9440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f5d28ac1e3fa414d9e7641f00ac514b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4af0ce7bac7f4a52b9b2088e63a99ce8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_00c2d42295c34646afa689f1695cbae8",
      "value": "â€‡718/718â€‡[00:00&lt;00:00,â€‡31.2kB/s]"
     }
    },
    "f628e363572b4becaec55d3e50613f6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
